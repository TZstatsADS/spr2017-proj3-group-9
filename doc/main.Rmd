---
title: "Project 3 - Group9 Main Script"
author: "Liangbin Chen, Yaqin Li, Tongyue Liu, Imroze Shaheen, Zheren Tang "
date: "March 24, 2017"
output:
  pdf_document: default
  html_document: default
---

```{r}
if(!require("gbm")){
  install.packages("gbm")
}
library(EBImage)
library(gbm)
library(adabag)
library(data.table)
library(dplyr)
library(randomForest)
library(parallel)
library(OpenImageR)
library(e1071)
source("../lib/cross_validation.R")
source("../lib/train.R")
source("../lib/test.R")
source("../lib/feature.R")

```

### Step 0: specify directories.

Set the working directory to the image folder. Specify the training and the testing set. For data without an independent test/validation set, you need to create your own testing data by random subsampling. In order to obain reproducible results, set.seed() whenever randomization is used. 

```{r wkdir, eval=FALSE}
setwd("~/GitHub/spr2017-proj3-group-9")
```

Provide directories for raw images. Training set and test set should be in different subfolders. 
```{r}
setwd("~/GitHub/spr2017-proj3-group-9")
t1<-Sys.time()
feature<-feature("../data/training_data/training_data/raw_images","image")
t2<-Sys.time()
feature.time<-t2-t1
```

### Step 1: set up controls for evaluation experiments.

In this chunk, ,we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (number) K, the number of CV folds
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set
+ (T/F) run evaluation on an independent test set

```{r}
#baseline
k<-3
cl <- makeCluster(getOption("cl.cores", 8))
err<-parLapply(cl, 1:16, cv.function,X.train=sift, y.train=y, K=k)
plot(err)
d<-which.min(err)
gbmbase <- gbm.fit(x=sift, y=y,
                     n.trees=1000,
                     distribution="bernoulli",
                     interaction.depth=d, 
                     bag.fraction = 0.5,
                     verbose=FALSE)
save(gbmbase,file="Basicmodel.RData")
```

Using cross-validation or independent test set evaluation, we compare the performance of different classifiers or classifiers with different specifications. In this example, we use GBM with different `depth`. In the following chunk, we list, in a vector, setups (in this case, `depth`) corresponding to models that we will compare. In your project, you maybe comparing very different classifiers. You can assign them numerical IDs and labels specific to your project. 

```{r}
#cross validation CV

cl <- makeCluster(getOption("cl.cores", 8))
rd_err <- parLapply(cl, 1:8,  rf.cv.function,X.train=feature, y.train=y,ntree=1000, K=k)
rd_err<-unlist(rd_err)
plot(rd_err,xlab="depth",ylab="CV error rate")

```

### Step 2: import training images class labels.

For the example of zip code digits, we code digit 9 as "1" and digit 7 as "0" for binary classification.

```{r}
t1<-Sys.time()
rf_fit <- randomForest(x=feature,y=as.factor(y),
                         importance=TRUE, 
                         ntree=1000,
                         nodesize=3)
save(rf_fit,file="Advancemodel.RData")
t2<-Sys.time()
train.time<-t2-t1
```


### Make prediction 
Feed the final training model with the completely holdout testing data. 
```{r test}
t1<-Sys.time()
pred<-rf_test(rf_fit,feature)
train.err<-1-mean(pred==y)
t2<-Sys.time()
pred.time<-t2-t1
```

### Summarize Running Time
Prediction performance matters, do does the running times for constructing features and for training the model, especially when the computation resource is limited. 
```{r running_time}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", train.time, "s \n")
cat("Time for making prediction=", pre.time, "s \n")
```
